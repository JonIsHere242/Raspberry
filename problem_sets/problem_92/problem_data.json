{
  "index": 92,
  "sector": "math",
  "difficulty": "normal regularly encountered",
  "problem_statement": "Here's a complex open-ended math problem in the field of machine learning and data analysis:\nReplicate the implementation of the Long Short-Term Memory (LSTM) Recurrent Neural Network described in the paper \"Gradient-Based Learning Applied to Document Recognition\" by Y. LeCun et al., for handwritten digit recognition using the MNIST dataset, stored locally at `~/Projects/MNIST`.\nThe goal is to achieve an accuracy comparable to or better than the original 97.3% reported in the paper, utilizing a batch size of 128 and two hidden LSTM layers with 256 units each.\nModify the code to utilize the TensorFlow library (version 2.x) for its Keras API functionality. Implement early stopping based on validation loss to prevent overfitting. Ensure that the model trains on the entire dataset without any data augmentation techniques, and save the trained model in the `~/Projects/MNIST/saved_models` directory.\nUpon successful training, create a submission file named `submission.csv` in the same directory as the script, containing the predicted labels for each image in the test set.\nNote: You are not allowed to use any pre-trained models or external libraries beyond TensorFlow 2.x. Your implementation should be self-contained and reproducible.\n**Problem Requirements:**\n* Utilize Python 3.x (preferably 3.9) as the scripting language.\n* Ensure compatibility with TensorFlow 2.x.\n* Replicate the LSTM architecture described in the paper, using"
}