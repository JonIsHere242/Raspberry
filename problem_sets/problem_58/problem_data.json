{
  "index": 58,
  "sector": "factual",
  "difficulty": "nearly impossible",
  "problem_statement": "Implement a natural-language-processing (NLP) system that can summarize long documents in various domains while maintaining semantic accuracy. The system should be able to recognize and incorporate nuanced language, idioms, and context-dependent expressions from the text.\nThe NLP system will process text files from the \"documents\" folder, which contains subfolders for different domains such as finance, science, history, etc. Each domain folder has a set of .txt files with long documents.\nUsing Python and the NLTK library, write a function that:\n- Tokenizes and parses the text into sentences\n- Identifies key phrases using named entity recognition (NER)\n- Applies sentiment analysis to determine the emotional tone of each sentence\n- Clusters similar sentences based on their content and sentiment\nThe system should produce a summary for each document in the form of a list of sentences, where each sentence is represented as a tuple containing the original text, key phrases, sentiment score, and cluster label.\nSave the summaries to a new .txt file in the \"summaries\" folder with the same name as the original document.\nThe system should be able to handle documents with varying lengths and complexities, and should be able to maintain semantic accuracy even when dealing with nuanced language and idioms.\n### Hint:\nYou can use the following libraries:\n- NLTK for tokenization and named entity recognition\n- spaCy for sentiment analysis\n- scikit-learn for clustering\nThe system should be able to handle documents with varying lengths and complexities"
}